{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "trainDf = pd.read_csv ('data/ML-CUP20-TR.csv')\n",
    "\n",
    "#trainNormalized = minmax_scale(trainDf, feature_range=(-1,1), axis=0)\n",
    "#trainDf = pd.DataFrame(trainNormalized)\n",
    "trainDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainX = trainDf.iloc[:, 1:10].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n",
    "    kmeans.fit(trainX)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('no of clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmodel = KMeans(n_clusters= 3, init='k-means++', random_state=0)\n",
    "y_kmeans= kmeansmodel.fit_predict(trainX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(trainX[y_kmeans == 0, 0], trainX[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(trainX[y_kmeans == 1, 0], trainX[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(trainX[y_kmeans == 2, 0], trainX[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.title('Clusters of Data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "trainDf = pd.read_csv ('data/ML-CUP20-TR.csv')\n",
    "#transformer = normalize().fit(trainDf.values)\n",
    "colnames=trainDf.columns.values\n",
    "#trainNormalized = normalize(trainDf.values, axis=0)\n",
    "trainDf=pd.DataFrame(minmax_scale(trainDf, feature_range=(0,1), axis=0))\n",
    "#trainDf = pd.DataFrame(transformer.transform(trainDf.values))\n",
    "trainDf.columns=colnames\n",
    "trainDf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(trainDf.corr(), annot=True,cmap=\"mako\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "trainDf['X6'].hist(bins=30)\n",
    "plt.axvline(trainDf['X6'].mean(), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()+trainDf['X6'].std(), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()+(2*trainDf['X6'].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()+(3*trainDf['X6'].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()-(trainDf['X6'].std()),   color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()-(2*trainDf['X6'].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(trainDf['X6'].mean()-(3*trainDf['X6'].std()), color='r', linestyle='dashed', linewidth=2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colsY1 = [\"X1\",\"X2\",\"X3\",\"X5\",\"X7\",\"X9\",\"Y1\"]\n",
    "sns.pairplot(trainDf[colsY1],height=2.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.hist(bins=50, figsize=(30,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "colsY1 = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"Y1\",\"Y2\"]\n",
    "for index,i in enumerate(colsY1):\n",
    "    \n",
    "    plt.figure(index)\n",
    "    sns.distplot(trainDf[i], fit=norm);\n",
    "\n",
    "    # Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(trainDf[i])\n",
    "   # print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.axvline(trainDf[i].mean(), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()+trainDf[i].std(), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()+(2*trainDf[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()+(3*trainDf[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()-(trainDf[i].std()),   color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()-(2*trainDf[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.axvline(trainDf[i].mean()-(3*trainDf[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    #Now plot the distribution\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Data distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = stats.zscore(trainDf)\n",
    "\n",
    "\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "new_df = trainDf[filtered_entries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "colsY1 = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\",\"X9\",\"X10\",\"Y1\",\"Y2\"]\n",
    "for index,i in enumerate(colsY1):\n",
    "    \n",
    "    plt.figure(index)\n",
    "    sns.distplot(new_df[i], fit=norm);\n",
    "\n",
    "    # Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(trainDf[i])\n",
    "   # print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.axvline(new_df[i].mean(), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean(),0,'mean')\n",
    "    plt.axvline(new_df[i].mean()+new_df[i].std(), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()+new_df[i].std(),0,\"1*std\")\n",
    "    plt.axvline(new_df[i].mean()+(2*new_df[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()+2*new_df[i].std(),0,\"2*std\")\n",
    "    plt.axvline(new_df[i].mean()+(3*new_df[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()+3*new_df[i].std(),0,\"3*std\")\n",
    "    plt.axvline(new_df[i].mean()-(new_df[i].std()),   color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()-new_df[i].std(),0,\"-1*std\")\n",
    "    plt.axvline(new_df[i].mean()-(2*new_df[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()-2*new_df[i].std(),0,\"-2*std\")\n",
    "    plt.axvline(new_df[i].mean()-(3*new_df[i].std()), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(new_df[i].mean()-3*new_df[i].std(),0,\"-3*std\")\n",
    "    #Now plot the distribution\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Data distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainDf.to_numpy()\n",
    "X = dataset[:, 0:10]\n",
    "Y = dataset[:, 10]\n",
    "\n",
    "X[0]\n",
    "\n",
    "#trainDf.isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(10, input_dim=10, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "x=dataset[:,0:10]\n",
    "print(x[0])\n",
    "y=dataset[:,10]\n",
    "y=np.reshape(y, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae',\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=5,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset[:,0:10]\n",
    "print(x[0])\n",
    "y=dataset[:,11]\n",
    "y=np.reshape(y, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(12, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(8, activation='relu'))\n",
    "model1.add(Dense(1, activation='linear'))\n",
    "model1.summary()\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=['mse','mae',\"accuracy\"])\n",
    "history1 = model1.fit(X_train, y_train, epochs=150, batch_size=5,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "print(history.history[\"mae\"][-1])\n",
    "print(history.history[\"val_mae\"][-1])\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history1.history.keys())\n",
    "# \"Loss\"\n",
    "print(history1.history[\"mae\"][-1])\n",
    "print(history1.history[\"val_mae\"][-1])\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss and accuracy')\n",
    "plt.ylabel('loss/accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validation_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset[:,0:10]\n",
    "print(x[0])\n",
    "y=dataset[:,11:12]\n",
    "y=np.reshape(y, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(12, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "model2.add(Dense(2, activation='linear'))\n",
    "model2.summary()\n",
    "model2.compile(loss='mse', optimizer='adam', metrics=['mse','mae',\"accuracy\"])\n",
    "history2 = model2.fit(X_train, y_train, epochs=150, batch_size=5,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history2.history.keys())\n",
    "print(history2.history[\"mae\"][-1])\n",
    "print(history2.history[\"val_mae\"][-1])\n",
    "# summarize history for accuracy\n",
    "plt.plot(history2.history['mae'])\n",
    "plt.plot(history2.history['val_mae'])\n",
    "plt.title('model mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "print(history2.history[\"mae\"][-1])\n",
    "print(history2.history[\"val_mae\"][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
