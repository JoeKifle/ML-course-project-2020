{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.gridseach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-428187120afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridseach\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.gridseach'"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "#keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.gridseach import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#################TRAINING PARAM\n",
    "verbose_level=0\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# for combining all the hyper-parameters\n",
    "import itertools \n",
    "from time import time\n",
    "from functools import reduce \n",
    "import operator\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "from tensorflow.python.client import device_lib #If training machine has gpu limit to one core(GridSearchCv does not support multithreading and gpu use) else use all available cores\n",
    "tmp=device_lib.list_local_devices()\n",
    "counter=0\n",
    "jobs=-1\n",
    "for i in device_lib.list_local_devices():\n",
    "    if(i.device_type==\"GPU\"):\n",
    "        counter+=1\n",
    "if(counter!=0):\n",
    "    jobs=1\n",
    "#jobs=1 #due to the low dimension of the dataset the single gpu version is slower, so I'm forcing it to the multi core version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for the best parameter\n",
    "g_eta = [0.1,0.2,0.3]\n",
    "g_batchSize = [10,20,25,30]\n",
    "g_hiddeLayerunit1 = [4,8,10]\n",
    "g_momentum = [0.6,0.8,0.9]\n",
    "g_afHiddenLayerunit1 = [\"relu\", \"tanh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading training and testing data from the csv file\n",
    "trainDf = pd.read_csv ('data/csv/monks-3Train.csv',header=None).to_numpy()\n",
    "testDf = pd.read_csv ('data/csv/monks-3Test.csv',header=None).to_numpy()\n",
    "trainDf = minmax_scale(trainDf, feature_range=(0,1), axis=0)\n",
    "testDf = minmax_scale(trainDf, feature_range=(0,1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing trainDf for training\n",
    "trainX = trainDf[:, 1:7]\n",
    "trainY = trainDf[:, 0]\n",
    "#trainX = scaler.fit_transform(trainX)\n",
    "#trainY = scaler.fit_transform(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing testDf for validation\n",
    "testX = testDf[:, 1:7]\n",
    "testY = testDf[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All combination of the hyper-paramers\n",
    "gridParameters=[g_eta,g_batchSize,g_hiddeLayerunit1,g_momentum,g_afHiddenLayerunit1]\n",
    "allcombination = list(itertools.product(*gridParameters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grid search I was able to get the best hyperparameters\n",
    "eta = 0.1\n",
    "batch size = 10\n",
    "hiddenLayer = 10\n",
    "momentum = 0.7\n",
    "hiddenLayer af = relu\n",
    "\n",
    "(0.1, 10, 10, 0.9, 'relu') - Then added a new hidden layer with activation unit of tanh and get accuracy of 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "tensorboard_callback = ks.callbacks.TensorBoard(log_dir=\"./log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE\n",
      "HEHE\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\branch-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'fit_params'"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "def create_model(lr=0.1,momentum=0.9,g_hiddeLayerunit1=4,g_hiddeLayerunit2=4,activation1=\"tanh\",activation2=\"tanh\",activation3=\"sigmoid\",init_mode='uniform', g_decay=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(g_hiddeLayerunit1, input_dim=6, kernel_initializer=init_mode, activation=activation1))\n",
    "    model.add(Dropout(0.4)),\n",
    "    model.add(Dense(g_hiddeLayerunit2, kernel_initializer=init_mode, activation=activation2,kernel_regularizer=regularizers.l1(0.01))) # l1 regularization parameter could also be something we can also grid search for.\n",
    "    model.add(Dropout(0.4)),\n",
    "    model.add(Dense(1, activation3))\n",
    "    sgd = SGD(lr=lr, momentum=momentum, decay=g_decay, nesterov=False,)  # We can add decay to hyper parameter list to get optimum value. \n",
    "    model.compile(optimizer=sgd, loss='mean_squared_error',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class KerasRegressorTB(KerasClassifier):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(KerasClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, x, y, log_dir=None, **kwargs):\n",
    "        cbs = None\n",
    "        if log_dir is not None:\n",
    "            # Make sure the base log directory exists\n",
    "            try:\n",
    "                os.makedirs(log_dir)\n",
    "            except OSError:\n",
    "                pass\n",
    "            params = self.get_params()\n",
    "            conf = \",\".join(\"{}={}\".format(k, params[k])\n",
    "                            for k in sorted(params))\n",
    "            conf_dir_base = os.path.join(log_dir, conf)\n",
    "            # Find a new directory to place the logs\n",
    "            for i in itertools.count():\n",
    "                try:\n",
    "                    conf_dir = \"{}_split-{}\".format(conf_dir_base, i)\n",
    "                    os.makedirs(conf_dir)\n",
    "                    break\n",
    "                except OSError:\n",
    "                    pass\n",
    "            cbs = [ks.callbacks.TensorBoard(log_dir=conf_dir, histogram_freq=0,\n",
    "                               write_graph=True, write_images=False)]\n",
    "        super(KerasRegressorTB, self).fit(x, y, callbacks=cbs, **kwargs)\n",
    "\n",
    "        \n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "g_hiddeLayerunit1=[4,8,16]\n",
    "g_hiddeLayerunit2=[4,8,16]\n",
    "g_decay = [0.01] # more estimations can be added\n",
    "activation1 = [\"relu\", \"tanh\",\"sigmoid\",\"softmax\"]\n",
    "activation2 = [\"relu\", \"tanh\",\"sigmoid\",\"softmax\"]\n",
    "activation3 = [\"relu\", \"tanh\",\"sigmoid\",\"softmax\"]\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "lr=np.arange(0.1, 0.9, 0.1).tolist()\n",
    "momentum=np.arange(0.1, 0.9, 0.1).tolist()\n",
    "param_grid = dict(\n",
    "    lr=lr,\n",
    "   # g_hiddeLayerunit1=g_hiddeLayerunit1,\n",
    "   # g_hiddeLayerunit2=g_hiddeLayerunit2,\n",
    "   # activation1=activation1,\n",
    "   # activation2=activation2,\n",
    "   # activation3=activation3,\n",
    "   # batch_size=batch_size,\n",
    "   # init_mode=init_mode,\n",
    "   # g_decay = g_decay,\n",
    "   # epochs=epochs\n",
    "    )\n",
    "model = KerasRegressorTB(build_fn=create_model)\n",
    "print(\"HE\")\n",
    "\n",
    "\n",
    "print(\"HEHE\")\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,fit_params={'log_dir': './Graph'},n_jobs=1,cv=3,verbose=verbose_level)\n",
    "\n",
    "\n",
    "print(\"HEHEHE\")\n",
    "grid_result = grid.fit(trainX, trainY)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Mean\\tSTD\\tParams\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12088), started 1:39:20 ago. (Use '!kill 12088' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-247b0e0795052220\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-247b0e0795052220\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
